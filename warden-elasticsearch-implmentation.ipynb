{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elasticsearch & Warden\n",
    "This notebook will illustrate how to organize our data in elasticsearch.  \n",
    "\n",
    "First, have a local version of elasticsearch running.\n",
    "  \n",
    "  \n",
    "Lets connect to our elasticsearch and test the connection.  \n",
    "Run this cell before using the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Testing connection...\nSuccess!\n"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "\n",
    "ES = Elasticsearch()\n",
    "\n",
    "print('Testing connection...')\n",
    "if ES.ping():\n",
    "    print('Success!')\n",
    "else:\n",
    "    print('No connection...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index creation  \n",
    "  \n",
    "The first question we need to answer is *What will we put in our index*? We used to drop whole states of every endpoint in an organization in daily indices. However, the list of pii files contained in the state object is an unbounded list, meaning it could grow to whatever size. This made the state object have an unlimited number of nested objects. A large number of nested objects per document (> 10k) can cause problem in elasticsearch. This limit can be modified but in this interest of robustness, scalability and best practices, we will instead post **individual files** in our index.  \n",
    "  \n",
    "We now define the mapping that will represent a file object in our index.  \n",
    "Each file that was posted as part of a single state will have the same **state** parameter, this will enable us to retrieve the content of a state with ease.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {\n",
    "    'dynamic': 'strict',  # Makes it so the index will reject data if it does not respect the mapping\n",
    "    'properties': {\n",
    "        'path': {'type': 'text'},\n",
    "        'score': {'type': 'float'},\n",
    "        'mime_type': {'type': 'keyword'},\n",
    "        'hash': {'type': 'keyword'},\n",
    "        'encrypted': {'type': 'boolean'},\n",
    "        'timestamp': {'type': 'date'},\n",
    "        'content': {\n",
    "            'type': 'nested',\n",
    "            'properties': {\n",
    "                'type_name': {'type': 'text'},    # This level represents the content of a pii file\n",
    "                'type_id': {'type': 'keyword'},   # Type name and id represent the name and id of the corresponding RegExes\n",
    "                'amount': {'type': 'integer'},\n",
    "                'correlations': {\n",
    "                    'type': 'nested',\n",
    "                    'properties': {\n",
    "                        'type_name': {'type': 'text'},\n",
    "                        'type_id': {'type': 'keyword'},              # This level represents the correlation level\n",
    "                        'correlation': {'type': 'float'}       # This structure allows for correlation on multiple entities, not just names\n",
    "                        }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        'state': {\n",
    "            'properties': {\n",
    "                'timestamp': {'type': 'date'},\n",
    "                'endpoint_id': {'type': 'keyword'},\n",
    "                'build_number': {'type': 'keyword'},\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The settings parameter remains to be fine tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"number_of_shards\": 1, \n",
    "    \"max_inner_result_window\": 1000, # Number of top hits we can pull from an aggregation\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next questions we need to answer are:\n",
    "\n",
    "-*When do we create a new index*?\n",
    "\n",
    "\n",
    " -- to figure out --\n",
    "\n",
    "\n",
    "-*How do we format the names of our indices*?  \n",
    "\n",
    "We need to be able to identify to which organization the new index belongs so we will add an organization identifier in the name. We will also need to know what time period is contained within the index, so we will add a date in it. Elasticsearch allows queries on wildcard strings for indices, so by using a format such as **mondata-{date}**, we can easily fetch all indices for the same organization by querying ** mondata-* **. Also, elasticsearch allows for date maths on queries, so we can easily query for specific time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Attempting to create index: mondata-1588692159917\nResponse:  {'acknowledged': True, 'shards_acknowledged': True, 'index': 'mondata-1588692159917'}\n"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "org_id = 'mondata'\n",
    "created_at = int(round(time.time()*1000, 0)) # ms since epoch\n",
    "\n",
    "index_name = f'{org_id}-{created_at}'\n",
    "request_body = {'mappings': mappings, 'settings': settings}\n",
    "\n",
    "print(f'Attempting to create index: {index_name}')\n",
    "\n",
    "response = ES.indices.create(index=index_name, body=request_body)\n",
    "\n",
    "print('Response: ', response)\n",
    "\n",
    "index_id = response['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells can be used for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_id = 'mondata-1588692159917' # If the notebook had to be restarted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES.indices.put_mapping(mappings, index=index_id) # If you need to change the mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'acknowledged': True}"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "# ES.delete(index=index_id, id='PqdMVnEBD8xa8m9A2jYh')\n",
    "# ES.delete_by_query(index=index_id, body={\"query\": {\"match_all\": {}}})\n",
    "# ES.indices.delete(index=index_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'took': 0,\n 'timed_out': False,\n '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0},\n 'hits': {'total': {'value': 0, 'relation': 'eq'},\n  'max_score': None,\n  'hits': []}}"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "# ES.search(index=index_id, body={'query': {'match_all': {}}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data  \n",
    "  \n",
    "The process for uploading data is pretty straight forward. We wrote code to generate realistic and scalable test data. This makes it easy to test queries on the model and test with various loads.  \n",
    "  \n",
    "Here we generate a **post** object that contains a series of files. This is what we want to upload in our index.  \n",
    "  \n",
    "Since our mapping has parameter 'dynamic':'strict', ES will reject our data if it doesnt fit the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'took': 36,\n 'errors': False,\n 'items': [{'index': {'_index': 'mondata-1588692159917',\n    '_type': '_doc',\n    '_id': 'I3vx5XEBaKK3TgPP7Dur',\n    '_version': 1,\n    'result': 'created',\n    '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n    '_seq_no': 6,\n    '_primary_term': 1,\n    'status': 201}},\n  {'index': {'_index': 'mondata-1588692159917',\n    '_type': '_doc',\n    '_id': 'JHvx5XEBaKK3TgPP7Dur',\n    '_version': 1,\n    'result': 'created',\n    '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n    '_seq_no': 7,\n    '_primary_term': 1,\n    'status': 201}},\n  {'index': {'_index': 'mondata-1588692159917',\n    '_type': '_doc',\n    '_id': 'JXvx5XEBaKK3TgPP7Dur',\n    '_version': 1,\n    'result': 'created',\n    '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n    '_seq_no': 8,\n    '_primary_term': 1,\n    'status': 201}},\n  {'index': {'_index': 'mondata-1588692159917',\n    '_type': '_doc',\n    '_id': 'Jnvx5XEBaKK3TgPP7Dur',\n    '_version': 1,\n    'result': 'created',\n    '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n    '_seq_no': 9,\n    '_primary_term': 1,\n    'status': 201}},\n  {'index': {'_index': 'mondata-1588692159917',\n    '_type': '_doc',\n    '_id': 'J3vx5XEBaKK3TgPP7Dur',\n    '_version': 1,\n    'result': 'created',\n    '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n    '_seq_no': 10,\n    '_primary_term': 1,\n    'status': 201}},\n  {'index': {'_index': 'mondata-1588692159917',\n    '_type': '_doc',\n    '_id': 'KHvx5XEBaKK3TgPP7Dur',\n    '_version': 1,\n    'result': 'created',\n    '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n    '_seq_no': 11,\n    '_primary_term': 1,\n    'status': 201}}]}"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "from file import File\n",
    "from post import Post\n",
    "\n",
    "post = Post(amount_of_files=6, endpoint_index=0)\n",
    "\n",
    "# The bulk api requires a particular format [ {action}, {object}, {action}, {object}, ... ]\n",
    "temp_list = [ [{'index': {'_index': index_id}}, f.json()] for f in post.files ]\n",
    "body = [val for sublist in temp_list for val in sublist]\n",
    "\n",
    "ES.bulk(body=body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying an index  \n",
    "  \n",
    "Here we detail the process of querying the state of an endpoint at a specific point in time.  \n",
    "  \n",
    "We start by querying every file before our reference timestamp that has the right *state.endpoint_id*. Next we run a term aggregation bucket on *state.timestamp* to regroup files that were posted together as part of a single state. We get the top hits and the maximum *score* from this bucket. This represent the most sensitive files in the state and the sensitivity score of the endpoint at that time.  \n",
    "\n",
    "The *doc_count* parameter in the response is the total number of files that belong in the state that were found in the index. If the *top_hits* aggregation returned less than this number, we have an incomplete state. This is not a bad thing as a state can get quite large. However, the results are sorted by *score* so the first files returned will be the most sensitive ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_as_of = 1986777120284\n",
    "size = 100\n",
    "\n",
    "query = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'filter': [\n",
    "                { \"term\":  { \"state.endpoint_id\": 'Nics endpoint' }},\n",
    "                { \"range\": { \"state.timestamp\": { \"lte\":  files_as_of }}},\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    'size': 0,\n",
    "    'aggregations': {\n",
    "        'states': {\n",
    "            'terms': {'field': 'state.timestamp', 'size': 1, 'order': {'_key': 'desc'} },\n",
    "            'aggregations': {\n",
    "                'score': {\n",
    "                    'max': {'field': 'score'}\n",
    "                },\n",
    "                'files': {\n",
    "                    'top_hits': {\n",
    "                        'sort': [\n",
    "                            {\"score\": { \"order\": \"desc\"} },\n",
    "                        ],\n",
    "                        'size': size\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "res = ES.search(index=index_id, body=query)\n",
    "f = open(\"query_1.txt\", \"a\")\n",
    "f.write(json.dumps(res, sort_keys=True, indent=4, separators=[',', ':']))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will write a query to get the score histogram of an organization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM = 0\n",
    "TO = int(round(time.time()*1000, 0)) + 1000000000000\n",
    "timezone = '-05:00'\n",
    "interval = '1d'\n",
    "number_of_files = 3\n",
    "\n",
    "query = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'filter': [\n",
    "                {'range': {'state.timestamp': {'time_zone': timezone, 'gte': FROM, 'lte': TO}}}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    'size': 0,\n",
    "    'aggregations': {\n",
    "        'calendar_slices': {\n",
    "            'date_histogram': {\n",
    "                'field': 'state.timestamp',\n",
    "                'calendar_interval': interval,\n",
    "                'time_zone': timezone,\n",
    "            },\n",
    "            'aggregations': {\n",
    "                'score': {'max': {'field': 'score'}},\n",
    "                'most_sensitive_files': {\n",
    "                    'top_hits': {\n",
    "                        'sort': [\n",
    "                            {'score': {'order': 'desc'}}\n",
    "                        ],\n",
    "                        'size': number_of_files\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "res = ES.search(index=index_id, body=query)\n",
    "f = open(\"query_2.txt\", \"a\")\n",
    "f.write(json.dumps(res, sort_keys=True, indent=4, separators=[',', ':']))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for a single endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM = 0\n",
    "TO = int(round(time.time()*1000, 0)) + 1000000000000\n",
    "timezone = '-05:00'\n",
    "interval = '1d'\n",
    "number_of_files = 3\n",
    "endpoint_id = 'Nics endpoint'\n",
    "\n",
    "query = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'filter': [\n",
    "                { \"term\":  { \"state.endpoint_id\": endpoint_id }},\n",
    "                {'range': {'state.timestamp': {'time_zone': timezone, 'gte': FROM, 'lte': TO}}}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    'size': 0,\n",
    "    'aggregations': {\n",
    "        'calendar_slices': {\n",
    "            'date_histogram': {\n",
    "                'field': 'state.timestamp',\n",
    "                'calendar_interval': interval,\n",
    "                'time_zone': timezone,\n",
    "            },\n",
    "            'aggregations': {\n",
    "                'score': {'max': {'field': 'score'}},\n",
    "                'most_sensitive_files': {\n",
    "                    'top_hits': {\n",
    "                        'sort': [\n",
    "                            {'score': {'order': 'desc'}}\n",
    "                        ],\n",
    "                        'size': number_of_files\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "res = ES.search(index=index_id, body=query)\n",
    "f = open(\"query_2.txt\", \"a\")\n",
    "f.write(json.dumps(res, sort_keys=True, indent=4, separators=[',', ':']))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitbasecondada80d3bf84294e43992d3e0b50cef699",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}