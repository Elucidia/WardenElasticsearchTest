{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitbasecondada80d3bf84294e43992d3e0b50cef699",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elasticsearch & Warden\n",
    "This notebook will illustrate how to organize our data in elasticsearch.\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lets connect to our elasticsearch and test the connection.  \n",
    "Run this cell before using the notebook.\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Testing connection...\nSuccess!\n"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "\n",
    "ES = Elasticsearch()\n",
    "\n",
    "print('Testing connection...')\n",
    "if ES.ping():\n",
    "    print('Success!')\n",
    "else:\n",
    "    print('No connection...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an index\n",
    "Let's start by defining a mapping.  \n",
    "Mappings: https://www.elastic.co/guide/en/elasticsearch/reference/master/mapping.html#create-mapping  \n",
    "Data types: https://www.elastic.co/guide/en/elasticsearch/reference/master/mapping-types.html  \n",
    "  \n",
    "If you wish to modify the mapping, you can use the put_mapping cell below. No need to create a new index."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {\n",
    "    'dynamic': 'strict',  # Makes it so the index will reject data if it does not respect the mapping\n",
    "    'properties': {\n",
    "            'timestamp': {'type': 'date'},\n",
    "            'endpoint_id': {'type': 'keyword'},\n",
    "            'build_number': {'type': 'keyword'},\n",
    "\n",
    "            'pii_files': {\n",
    "                'type': 'nested',\n",
    "                'properties': {\n",
    "\n",
    "                    'path': {'type': 'text'},        # This level represents a list of pii files\n",
    "                    'score': {'type': 'float'},\n",
    "\n",
    "                    'mime_type': {'type': 'keyword'},\n",
    "                    'hash': {'type': 'keyword'},\n",
    "                    'encrypted': {'type': 'boolean'},\n",
    "                    'timestamp': {'type': 'date'},\n",
    "                    \n",
    "                    'content': {\n",
    "                        'type': 'nested',\n",
    "                        'properties': {\n",
    "\n",
    "                            'type_name': {'type': 'text'},    # This level represents the content of a pii file\n",
    "                            'type_id': {'type': 'keyword'},   # Type name and id represent the name and id of the corresponding RegExs\n",
    "                            'amount': {'type': 'integer'},\n",
    "                            'correlations': {\n",
    "                                'type': 'nested',\n",
    "                                'properties': {\n",
    "\n",
    "                                    'type_name': {'type': 'text'},\n",
    "                                    'type_id': {'type': 'keyword'},              # This level represents the correlation level\n",
    "                                    'correlation': {'type': 'float'}       # This structure allows for correlation on multiple entities, not just names\n",
    "                                }\n",
    "                            }\n",
    "                        }}\n",
    "                }}\n",
    "\n",
    "        }\n",
    "    }\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the settings of our index. The amount of resources we're going to allow a single index.  \n",
    "For now, we will allow a single shard by index. This setting cannot be changed. If we ever need to expand capacities, we can clone existing indices.  \n",
    "New indices can be created with different capacities.  \n",
    "Settings: https://www.elastic.co/guide/en/elasticsearch/reference/master/index-modules.html#index-modules-settings"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\"number_of_shards\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all that we need to create an index. We'll add the name of the org and the time of creation in the index name.  \n",
    "Creating an index: https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-create-index.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Attempting to create index: mondata-1586202184430\nResponse:  {'acknowledged': True, 'shards_acknowledged': True, 'index': 'mondata-1586202184430'}\n"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "org_id = 'mondata'\n",
    "created_at = int(round(time.time()*1000, 0)) # ms since epoch\n",
    "\n",
    "index_name = f'{org_id}-{created_at}'\n",
    "request_body = {'mappings': mappings, 'settings': settings}\n",
    "\n",
    "print(f'Attempting to create index: {index_name}')\n",
    "\n",
    "response = ES.indices.create(index=index_name, body=request_body)\n",
    "\n",
    "print('Response: ', response)\n",
    "\n",
    "index_id = response['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_id = 'mondata-1586202184430' # If the notebook had to be restarted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to modify the mapping of an index after creation.   \n",
    "putMapping: https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-put-mapping.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'acknowledged': True}"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "ES.indices.put_mapping(mappings, index=index_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data\n",
    "\n",
    "Let's generate some fake data. By using the state class we can generate realistic fake data to test our queries.  \n",
    "This will mimic 3 endpoints posting once a day for five days.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from state import *\n",
    "\n",
    "fake_data = get_state_timeserie(number_of_endpoints=3, states_per_endpoint=5) # format [ [state, ...], [state, ...], ... ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we put the data in our index. If the data structure corresponds to the mapping, the request will work. Else it will be rejected.  \n",
    "This is a result of setting the 'dynamic': 'strict'  parameter in our mapping.  \n",
    "Putting documents: https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-index_.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for timeseries in fake_data:\n",
    "    [ES.index(index=index_id, body=state.json()) for state in timeseries]\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data should be in our index. We can query for the content of the whole index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "data = ES.search(index=index_id, body={\"query\": {\"match_all\": {}}, \"size\": 10000})\n",
    "f = open(\"query_all_result.txt\", \"a\")\n",
    "f.write(json.dumps(data, sort_keys=True, indent=4, separators=[',', ':']))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can delete a document or all documents with the following lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ES.delete(index=index_id, id='9qeWSHEBD8xa8m9AXTWA')\n",
    "# ES.delete_by_query(index=index_id, body={\"query\": {\"match_all\": {}}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can upload data to our index however we want, we can start writting and testing queries.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the index  \n",
    "The first query we're interested in making is a simple query by endpoint_id. Since the endpoint_id field is mapped as a **keyword**, we can do a **term** query.  \n",
    "Term queries match to exact strings. We can also sort by timestamp and ask for a certain number of states.\n",
    "  \n",
    "Also, we're using the **filter** context. The filter context is prefered over the query context since it offers caching fonctionnality and should speed up performances.   \n",
    "The query context offers the relevance score, which we have no use for.  \n",
    "  \n",
    "Results from the queries are large, some queries we will save in text files.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = EndpointEnum[0]['name']\n",
    "last_x_states = 1 \n",
    "\n",
    "query = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'filter': [\n",
    "                { \"term\":  { \"endpoint_id\": endpoint }},\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    'size': last_x_states,\n",
    "    'sort': [{\n",
    "        'timestamp': {\n",
    "            'order': 'desc'\n",
    "        }\n",
    "    }]\n",
    "}\n",
    "\n",
    "res = ES.search(index=index_id, body=query)\n",
    "f = open(\"query_endpoint_id_result.txt\", \"a\")\n",
    "f.write(json.dumps(res, sort_keys=True, indent=4, separators=[',', ':']))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also query **nested fields**. In the next example, we will look for a particular file hash in our whole index.  \n",
    "We could also look for the same file hash on a particular endpoint. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'query': {\n",
    "        'nested': {\n",
    "            'path': 'pii_files',\n",
    "            'query': {\n",
    "                'bool': {\n",
    "                    'filter': [\n",
    "                        {\n",
    "                            'term': {'pii_files.hash': '33f75cfe-7842-11ea-9e8a-9cb6d08b03d4'} # This one you have to find by hand ...\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "res = ES.search(index=index_id, body=query)\n",
    "f = open(\"query_hash_result.txt\", \"a\")\n",
    "f.write(json.dumps(res, sort_keys=True, indent=4, separators=[',', ':']))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even do **double nested queries**. For exemple, we can look for every states with a file that has 100 or more instances of a specific type of sensitive information (type_id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_id = RegexEnum[0]['guid']\n",
    "amount = 100\n",
    "\n",
    "query = {\n",
    "    'query': {\n",
    "        'nested': {\n",
    "            'path': 'pii_files',\n",
    "            'query': {\n",
    "                'nested': {\n",
    "                    'path': 'pii_files.content',\n",
    "                    'query': {\n",
    "                        'bool': {\n",
    "                            'filter': [\n",
    "                                {\n",
    "                                    'term': {'pii_files.content.type_id': type_id}\n",
    "                                },\n",
    "                                {\n",
    "                                    'range': {'pii_files.content.amount': {'gte': 100}}\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "res = ES.search(index=index_id, body=query)\n",
    "f = open(\"query_type_id_result.txt\", \"a\")\n",
    "f.write(json.dumps(res, sort_keys=True, indent=4, separators=[',', ':']))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've estabilished that we can easily make the queries we want, let's attempt a few aggregations.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations  \n",
    "Let's try to compute the maximum score for an endpoint's latest state. We will run an aggregation on the pii_files field, and ask for the maximum score.  \n",
    "This is an exemple of a **nested aggregation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = EndpointEnum[0]['name']\n",
    "\n",
    "\n",
    "query = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'filter': [\n",
    "                { \"term\":  { \"endpoint_id\": endpoint }},\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    'aggregations': {\n",
    "        \"piifiles\" : {\n",
    "            'nested': {\n",
    "                'path': 'pii_files'\n",
    "            },\n",
    "            'aggregations': {\n",
    "                'max_score': {'max': {'field': \"pii_files.score\"}}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'size': 1,\n",
    "    'sort': [{\n",
    "        'timestamp': {\n",
    "            'order': 'desc'\n",
    "        }\n",
    "    }]\n",
    "}\n",
    "\n",
    "\n",
    "res = ES.search(index=index_id, body=query)\n",
    "f = open(\"aggs_max_score_result.txt\", \"a\")\n",
    "f.write(json.dumps(res, sort_keys=True, indent=4, separators=[',', ':']))\n",
    "f.close()"
   ]
  }
 ]
}